#!/usr/bin/env python3
import os
import re
import sys
import typing
from multiprocessing import Pool

pkg_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))  # noqa
sys.path.insert(0, pkg_root)  # noqa

from lib.config import Config
from lib.db.database import PostgresDatabase
from lib.etl.load import PostgresLoader, Loader
from lib.logger import logger
from lib.model import datetime_to_version
from lib.etl.extract import S3Client, S3Extractor, Extractor
from lib.etl.transform import BundleDocumentTransform

# this seems to be entirely IO bound, so we can have _lots_ of workers...
CONCURRENCY = 50

BUNDLE_FQID_REGEXP = re.compile(
    '^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}[.][0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{6}[.][0-9]{6}Z$',
    re.IGNORECASE
)


def chunks(seq, size):
    return (seq[i::size] for i in range(size))


def extract_transform_load(extractor: Extractor, loader: Loader, bundle_fqid: str):
    if not BUNDLE_FQID_REGEXP.match(bundle_fqid):
        logger.warn(f"Not a valid bundle with FQID: \"{bundle_fqid}\"")
        return
    try:
        bundle_uuid, bundle_version = bundle_fqid.split('.', 1)
        bundle = extractor.extract_bundle(bundle_uuid, bundle_version)
        transformed_bundle = BundleDocumentTransform.transform(bundle)
        loader.load(bundle, transformed_bundle)
        logger.info(f"Completed ETL for bundle with FQID: \"{bundle_fqid}\"")
    except:
        logger.exception(f"Could not load bundle with FQID: \"{bundle_fqid}\"")


def process_batch(batch: typing.List[str]):
    loader = PostgresLoader(PostgresDatabase(Config.serve_database_uri))
    # TODO: don't hardcode to staging
    # TODO: don't read directly from s3
    extractor = S3Extractor(S3Client('us-east-1', 'org-humancellatlas-dss-staging'))
    batch_len = len(batch)
    c = 0
    for bundle_fqid in batch:
        extract_transform_load(extractor, loader, bundle_fqid)
        c += 1
        if c % 10 == 0:
            logger.info(f"Batch completion: %0.02f%%" % (100.0 * float(c) / float(batch_len)))


if __name__ == '__main__':
    bundle_fqids = []
    for line in sys.stdin:
        bundle_fqids.append(line.strip())

    # eliminate fqids that have already been processed
    db = PostgresDatabase(Config.serve_database_uri)
    with db.transaction() as (cursor, _):
        cursor.execute("SELECT bundle_uuid, bundle_version from bundles;")
        already_processed_fqids = set(f"{ele[0]}.{datetime_to_version(ele[1])}" for ele in cursor.fetchall())

    logger.info(f"Number of Bundle FQIDS before filtering: {len(bundle_fqids)}")
    bundle_fqids = [fqid for fqid in bundle_fqids if fqid not in already_processed_fqids]
    logger.info(f"Number of Bundle FQIDS after filtering: {len(bundle_fqids)}")

    with Pool(CONCURRENCY) as p:
        bundle_fqid_chunks = chunks(bundle_fqids, CONCURRENCY)
        p.map(process_batch, bundle_fqid_chunks)

